{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of Filtering on Model Optimization\n",
    "---\n",
    "\n",
    "When incorporating a filtering process, such as a low-pass filter,like butterwoth into the data pre-processing step of model optimization, several key impacts need to be considered:\n",
    "\n",
    "### Noise Reduction\n",
    "- Filters are primarily utilized to eliminate noise from the dataset, e.g we applied buterworth filter to our data.\n",
    "- A well-designed filter can smooth out erratic fluctuations that do not correspond to the underlying system dynamics.\n",
    "- This leads to a clearer signal, helping the optimization algorithm to converge on the true parameter values more efficiently.\n",
    "\n",
    "### Signal Fidelity\n",
    "- While filtering suppresses noise, there's a risk of also attenuating important features of the signal.\n",
    "- It's essential to configure the filter to preserve the signal's integrity, ensuring that significant oscillations and trends are not lost.\n",
    "\n",
    "### Parameter Sensitivity\n",
    "- The optimization process relies on the gradient of the error with respect to the parameters.\n",
    "- By applying a filter, the gradients can become more pronounced for the true signal while reducing the influence of random variations.\n",
    "- Consequently, this can improve the sensitivity of the optimization algorithm to parameter changes, leading to more accurate parameter estimation.\n",
    "\n",
    "### Overfitting Considerations\n",
    "- Filtering might cause the model to overfit to the 'cleaned' data.\n",
    "- It's important to validate the model with unfiltered data to ensure that the predictive power of the model is not compromised.\n",
    "\n",
    "### Computational Efficiency\n",
    "- The application of a filter adds computational overhead.\n",
    "- However, by facilitating a smoother convergence and potentially reducing the number of iterations needed for optimization, the overall computational time might be decreased.\n",
    "\n",
    "In summary, filtering, when done correctly, can significantly enhance the optimization process. It improves the clarity of the data, which can aid in a more straightforward and potentially faster parameter estimation. Nonetheless, careful attention must be given to the filter design to ensure that it does not introduce artifacts or remove critical information from the signal. Cross-validation with both filtered and unfiltered data is recommended to ensure the robustness of the optimized model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could the grid search be improved? Try to implement a better/more advanced method?\n",
    "--\n",
    "\n",
    "This can be done by introducing Bayesian Optimization as a more advanced and efficient method for parameter tuning.\n",
    "Bayesian Optimization uses a probabilistic model to predict the performance of the parameters and then chooses the next parameters to evaluate by balancing exploration and exploitation. This method is more efficient than grid search,it intelligently selects which points to evaluate based on previous observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you change the code so that difference in sampling in the recoding and simulation is minimized?\n",
    "---\n",
    "\n",
    "\n",
    "i changed the code in a way that it uses the panda dataframe last row and first row, by substracting the 2 data point and the dividing it by the lenght of the total data frame len(df_time)\n",
    "\n",
    "```\n",
    "\n",
    "delta_t = (df_time.iloc[-1] - df_time.iloc[0]) / len(df_time)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What are the optimal parameters? And is the error acceptable? If not, could you include additional parameters in your model? What would they be and why do you think so?\n",
    "---\n",
    "\n",
    "Grid Search Optimal Parameters:\n",
    "\n",
    "```\n",
    "Air resistance coefficient (c_air): 0.17142857142857143 \n",
    "Coulomb friction coefficient (c_c): 0.46428571428571425\n",
    "Pendulum length (l):0.2888888888888889\n",
    "Grid Search Lowest Error: 0.1688847304185861\n",
    "```\n",
    "\n",
    "Bayesian Optimization Optimal Parameters:\n",
    "```\n",
    "\n",
    "Air resistance coefficient (c_air): 0.1662964167284311\n",
    "Coulomb friction coefficient (c_c): 0.49382918040251705\n",
    "Pendulum length (l):0.29465818542628064\n",
    "Bayesian Optimization Lowest Error: 0.2924939316211361\n",
    "```\n",
    "The errors from both Grid Search and Bayesian Optimization are modest (approximately 0.17 and 0.29). This level of error suggests a good fit between the simulation and the measured data, potentially suitable for various applications. However, the acceptability of these errors largely depends on specific requirements. High-precision tasks may demand even more stringent error margins.\n",
    "\n",
    "To refine the model further, I would consider adding parameters that characterize the motor's response, such as the motor acceleration force transfer coefficient (a_m), motor inertia, and electrical resistance. These parameters could significantly improve the accuracy of the simulation in systems where motor dynamics play a crucial role, by more precisely capturing the interplay between the pendulum's motion and the motor's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing a model can be computationally expensive. Discuss how you could further minimize the computation you would need for the optimization. Could you reduce the data you use? Could you evaluate in a sparse way, e.g. at random points? Could you automatically estimate the upper and lower range of the parameter values?\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It is computationally intensive, but there are strategies to manage and potentially reduce the computational burden in our model.\n",
    "\n",
    " To further minimize the computation required for optimization, reducing the dataset used for model training and validation is a viable strategy, where:\n",
    "\n",
    "Selective Data Sampling: Instead of using the entire dataset, I can employ techniques like random sampling or stratified sampling to create a smaller, yet representative, subset of the data. This approach maintains the diversity of the dataset while significantly reducing the amount of data to be processed.\n",
    "\n",
    "Focus on Key Phases: By analyzing the pendulum's behavior, I identified specific phases of motion (e.g., the initial release, steady oscillation, and damping) that are critical for model accuracy. Focusing on these key phases and using data that captures these aspects can enhance model optimization with less data.\n",
    "\n",
    "Data Windowing: Implementing a windowing technique allows for the examination of short segments of data that capture significant dynamics of the system. By sliding this window across the dataset and selectively choosing windows that show a range of behaviors, I can effectively reduce the amount of data while ensuring comprehensive model training.\n",
    "\n",
    "Reducing the data used for optimization can drastically decrease computational demands, making the process more efficient without compromising the model's ability to accurately replicate the pendulum's dynamics. However, itâ€™s crucial to ensure that the reduced dataset still captures the full range of pendulum behaviors to maintain the integrity and accuracy of the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `import numpy as np`: This line imports the numpy library and gives it the alias np. Numpy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "2. `import pandas as pd`: This line imports the pandas library and gives it the alias pd. Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
    "\n",
    "3. `import matplotlib.pyplot as plt`: This line imports the pyplot submodule from matplotlib and gives it the alias plt. Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy.\n",
    "\n",
    "4. `from Digital_twin import DigitalTwin`: This line imports the DigitalTwin class from the Digital_twin module. A digital twin is a digital replica of a living or non-living physical entity.\n",
    "\n",
    "5. `from bayes_opt import BayesianOptimization`: This line imports the BayesianOptimization class from the bayes_opt module. Bayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives.\n",
    "\n",
    "6. `digital_twin = DigitalTwin()`: This line creates an instance of the DigitalTwin class.\n",
    "\n",
    "7. `csv_file_path = 'filtered_data.csv'`: This line defines the path to the CSV file that contains the data.\n",
    "\n",
    "8. `df = pd.read_csv(csv_file_path)`: This line reads the CSV file at the specified path into a pandas DataFrame.\n",
    "\n",
    "9. `df_time = df['time_seconds']`: This line extracts the 'time_seconds' column from the DataFrame into a separate variable.\n",
    "\n",
    "10. `df_theta = df['theta_filtered']`: This line extracts the 'theta_filtered' column from the DataFrame into a separate variable.\n",
    "\n",
    "11. `#df_theta = np.radians(df_theta)`: This line, if uncommented, would convert the theta values from degrees to radians.\n",
    "\n",
    "12. `def find_initial_state(df_theta, df_time)`: This function calculates the initial state of the system by finding the initial theta and theta_dot values.\n",
    "\n",
    "13. `theta, theta_dot = find_initial_state(df_theta, df_time)`: This line calls the find_initial_state function and assigns the returned values to theta and theta_dot.\n",
    "\n",
    "14. `print(theta_dot)`: This line prints the value of theta_dot.\n",
    "\n",
    "15. `delta_t = (df_time.iloc[-1] - df_time.iloc[0]) / len(df_time)`: This line calculates the time step for the simulation.\n",
    "\n",
    "16. `print(delta_t)`: This line prints the value of delta_t.\n",
    "\n",
    "17. `sim_time = (12,25)`: This line defines the simulation time.\n",
    "\n",
    "18. `c_air_range = np.linspace(0, 0.4, 15)`: This line defines the range of values for the c_air parameter.\n",
    "\n",
    "19. `c_c_range = np.linspace(0, 0.5, 15)`: This line defines the range of values for the c_c parameter.\n",
    "\n",
    "20. `g = 9.81`: This line defines the value of the gravitational constant.\n",
    "\n",
    "21. `l_range = np.linspace(0.2, 0.4, 10)`: This line defines the range of values for the l parameter.\n",
    "\n",
    "22. `def simulate_potential_model(theta, theta_dot, c_air, c_c, g, l, num_steps)`: This function simulates the pendulum motion based on the digital twin model.\n",
    "\n",
    "23. `best_params = None`: This line initializes the variable that will hold the best parameters found during the grid search.\n",
    "\n",
    "24. `lowest_error = float('inf')`: This line initializes the variable that will hold the lowest error found during the grid search.\n",
    "\n",
    "25. The following lines perform a grid search over the parameter space to find the best parameters that minimize the error.\n",
    "\n",
    "26. `print(\"GRID Best Parameters:\", best_params)`: This line prints the best parameters found during the grid search.\n",
    "\n",
    "27. `print(\"GRID Lowest Error:\", lowest_error)`: This line prints the lowest error found during the grid search.\n",
    "\n",
    "28. `def optimize_parameters(c_air, c_c, l)`: This function defines the objective function for the Bayesian optimization.\n",
    "\n",
    "29. `pbounds = {'c_air': (0, 0.4), 'c_c': (0, 0.5), 'l': (0.2, 0.4)}`: This line defines the bounds for the parameters for the Bayesian optimization.\n",
    "\n",
    "30. `optimizer = BayesianOptimization(f=optimize_parameters, pbounds=pbounds, random_state=1)`: This line initializes the Bayesian optimizer.\n",
    "\n",
    "31. `optimizer.maximize(init_points=15, n_iter=30)`: This line starts the Bayesian optimization process.\n",
    "\n",
    "32. `bayesian_best_params = optimizer.max['params']`: This line retrieves the best parameters found by the Bayesian optimization.\n",
    "\n",
    "33. `bayesian_lowest_error = -optimizer.max['target']`: This line retrieves the lowest error found by the Bayesian optimization.\n",
    "\n",
    "34. `print(\"Bayesian Best Parameters:\", bayesian_best_params)`: This line prints the best parameters found by the Bayesian optimization.\n",
    "\n",
    "35. `print(\"Bayesian Lowest Error:\", bayesian_lowest_error)`: This line prints the lowest error found by the Bayesian optimization.\n",
    "\n",
    "36. The following lines simulate the system using the best parameters found by both the grid search and the Bayesian optimization.\n",
    "\n",
    "37. The following lines plot the measured data, the data simulated using the best parameters found by the grid search, and the data simulated using the best parameters found by the Bayesian optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
